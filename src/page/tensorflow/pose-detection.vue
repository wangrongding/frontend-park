<script lang="ts" setup>
import { PoseDetector } from '@tensorflow-models/pose-detection'
import * as poseDetection from '@tensorflow-models/pose-detection'
import '@tensorflow/tfjs-backend-webgl'

let loading = $ref(false)
let posenetInput: HTMLVideoElement | HTMLImageElement | HTMLCanvasElement
let posenetOutput: HTMLCanvasElement
let posenetOutputCtx: CanvasRenderingContext2D
let detector: PoseDetector
let model: poseDetection.SupportedModels.PoseNet

// const model = poseDetection.SupportedModels.MoveNet
// åˆå§‹åŒ–
const init = async () => {
  loading = true
  // è·å– canvas å…ƒç´ 
  posenetOutput = document.getElementById('output') as HTMLCanvasElement
  posenetOutputCtx = posenetOutput.getContext('2d')!
  // è·å–è§†é¢‘æµ
  posenetInput = document.getElementById('video') as HTMLVideoElement
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: true,
  })
  posenetInput.srcObject = stream
  // å®šä¹‰æ¨¡å‹
  model = poseDetection.SupportedModels.PoseNet
  // åŠ è½½æ¨¡å‹
  detector = await poseDetection.createDetector(model, {
    // æœ‰ä¸‰ç§æ¨¡å‹çš„ç±»å‹å¯ä¾›é€‰æ‹©ï¼šâ€liteâ€ã€â€full â€œå’Œ â€œheavyâ€ã€‚è¿™äº›æ”¹å˜äº†æ£€æµ‹æ¨¡å‹çš„å¤§å°ã€‚Liteçš„æ£€æµ‹ç²¾åº¦æœ€ä½ï¼Œä½†æ€§èƒ½æœ€å¥½ï¼Œè€Œ â€œheavy â€œçš„æ£€æµ‹ç²¾åº¦æœ€å¥½ï¼Œä½†æ›´æ¶ˆè€—æ€§èƒ½ï¼Œè€Œ â€œfull â€œåˆ™å¤„äºä¸­é—´ä½ç½®ã€‚æˆ‘ä»¬é€‰æ‹©äº†å®ƒ ã€‚
    modelType: 'full',
  })
  loading = false
  detectPose()
}

const canvas = document.createElement('canvas')
canvas.width = 360
canvas.height = 270

// å¼€å§‹æ£€æµ‹
const detectPose = async () => {
  const ctx = canvas.getContext('2d')!
  ctx.drawImage(posenetInput, 0, 0, 360, 270)
  const poses = await detector.estimatePoses(canvas, {
    flipHorizontal: false,
    maxPoses: 1,
    scoreThreshold: 0.5,
    nmsRadius: 20,
  })
  // console.log('ğŸš€ğŸš€ğŸš€ / poses', poses)

  // å°† pose ä¸Šçš„ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯å­˜å…¥ pointList
  const pointList = poses[0]?.keypoints || []
  // console.log('ğŸš€ğŸš€ğŸš€ /  pointList ', pointList)
  // å°†è¿™ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯ ç”»åˆ° canvas ä¸Š
  // posenetOutputCtx.clearRect(0, 0, canvas.width, canvas.height)
  posenetOutputCtx.drawImage(posenetInput, 0, 0, canvas.width, canvas.height)

  // ç”»å‡ºæ‰€æœ‰å…³é”®ç‚¹
  pointList.forEach(({ x, y, score, name }: any) => {
    if (score > 0.5) {
      // ç”»ç‚¹
      drawPoint(x, y, 5, '#f00000', posenetOutputCtx)
    }
  })
  // è·å–ç›¸é‚»çš„å…³é”®ç‚¹ä¿¡æ¯
  const adjacentPairs = poseDetection.util.getAdjacentPairs(model)
  // ç”»å‡ºæ‰€æœ‰è¿çº¿
  adjacentPairs.forEach(([i, j]: any) => {
    const kp1 = pointList[i]
    const kp2 = pointList[j]
    // score ä¸ä¸ºç©ºå°±ç”»çº¿
    const score1 = kp1.score != null ? kp1.score : 1
    const score2 = kp2.score != null ? kp2.score : 1
    if (score1 >= 0.5 && score2 >= 0.5) {
      // ç”»çº¿æ®µ
      drawSegment([kp1.x, kp1.y], [kp2.x, kp2.y], 'aqua', 1, posenetOutputCtx)
    }
  })
  // requestAnimationFrame(() => detectPose(detector))
  setTimeout(() => {
    detectPose()
  }, 50)
}

// ç”»ç‚¹
function drawPoint(x: number, y: number, r: number, color: string, ctx: CanvasRenderingContext2D) {
  ctx.beginPath()
  ctx.arc(x, y, r, 0, 2 * Math.PI)
  ctx.fillStyle = color
  ctx.fill()
}
// ç”»çº¿æ®µ
function drawSegment([ax, ay]: number[], [bx, by]: number[], color: string, scale: number, ctx: CanvasRenderingContext2D) {
  ctx.beginPath()
  ctx.moveTo(ax * scale, ay * scale)
  ctx.lineTo(bx * scale, by * scale)
  ctx.lineWidth = 6
  ctx.strokeStyle = color
  ctx.stroke()
}

onMounted(async () => {
  await init()
})
</script>
<template>
  <div v-loading="loading" class="h-[500px] text-center flex justify-center items-center gap-[20px]">
    <div>
      <span>input</span>
      <video id="video" autoplay playsinline muted class="w-[360px] h-[270px] object-fill"></video>
    </div>
    <!-- output -->
    <div>
      <span>output</span>
      <canvas id="output" width="360" height="270"></canvas>
    </div>
  </div>
</template>
<style lang="scss" scoped></style>
